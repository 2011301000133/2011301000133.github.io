---
layout: post
title: spark用法速查.
categories:
tags: 1数据准备
keywords:
description:
order: 151
---

## Spark与Hive的交互
### Hive to Spark
```py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test").enableHiveSupport().getOrCreate()
df_data = spark.sql('select ...') #返回spark下的DataFrame类型
```
### Spark to Hive
```py
df_data.toDF('col_name1','col_name2').registerTempTable("table3")
spark.sql("create table tmp.tmp_test_pyspark2hive as select * from table3")
# table3可以作为一个表用了，因此还有众多可以写sql的用法
```

## Spark与pandas的交互
### spark to pandas

```py
df_data.toPandas() #返回DataFrame类型
```
### pandas to spark
```py
spark.createDataFrame(<pd.DataFrame>)
```




## 输出到本地硬盘
```py
<pd.DataFrame>.to_csv('') #pandas 存储到本地csv
os.system("hive -e 'select * from ...'>test.csv") #hive存储到本地csv
```


## 教程
### 1. 初始化SparkContext
```py
from pyspark import SparkContext, SparkConf
conf=SparkConf().setMaster('local').setAppName('My App')
sc=SparkContext(conf=conf)
```

### 2. 创建RDD
```py
lines=sc.parallelize(['pandas','i like pandas','hello','hello world!','error line','warning line'])
lines=sc.textFile('test.txt')
```


RDD有两种操作：**transformation**， **actions**
### 3. transformation


```py
RDD1.map(func)#将结果作为新RDD的值
RDD1.filter(func)#筛选满足条件的RDD值
RDD1.flatMap(func) #如果func返回的是一个迭代器，那么把迭代器里所有对象放入新RDD中并摊平

#集合操作
RDD1.distinct()
RDD1.union(RDD2) #并集
RDD1.intersection(RDD2) #交集
RDD1.subtract(RDD2) #差集

RDD1.cartesian(RDD2) #笛卡尔积

RDD1.sample(withReplacement=False,fraction=0.2) #采样
```

例子：
```py
errorsRDD=lines.filter(lambda line:'error' in line)
warningRDD=lines.filter(lambda line:'warning' in line)

badRDD=errorsRDD.union(warningRDD)# 合并
```


### 4. actions

```py
RDD1.reduce(lambda x,y:x+y)
RDD1.fold(0,lambda x,y:x+y) #相当于一个带初始值的reduce
RDD1.foreach(func1) #对每个元素使用func1
RDD1.aggregate(zeroValue,seqOp,combOp)# 没搞太清楚

RDD1.collect()
RDD1.take(10)
RDD1.top(10)
RDD1.first()

RDD1.count()
RDD1.countByValue()
```



### pair RDD

pair RDD是一种特殊的RDD  
转化：  
```py
pair_RDD=sc.parallelize([(1,2),(5,2),(3,4),(3,3)])

pair_RDD.reduceByKey(func) #分组应用func,是transformation(reduce是actions)，例如func=lambda x,y:x+y
pari_RDD.foldByKey(0，func) #类似fold，带初始值的reduce
pair_RDD.groupByKey() # 同一个key下的value变成[value1,value2,...]？？？没明白
combineByKey
pair_RDD.flatMapValues(func) #对每个值应用func，key不变，返回iterable
pair_RDD.mapValues(func) #对每个value应用func，key保持不变


pair_RDD.keys()
pair_RDD.values()
pair_RDD.sortByKey()
```
双pair RDD操作：
```py
pair_RDD1=sc.parallelize([(1,2),(5,2),(3,4),(3,3)])
pair_RDD2=sc.parallelize([(1,5),(6,2)])

pair_RDD1.subtractByKey(pair_RDD2) #删除key相同的元素
pair_RDD1.join(pair_RDD2)  #内连接，返回类似[(1, (2, 5))]的RDD
pair_RDD1.leftOuterJoin(pair_RDD2) #左边的保留
pair_RDD1.rightOuterJoin(pair_RDD2) #右边的保留
pair_RDD1.cogroup(pair_RDD2) #key-value的RDD，其中value是一个iterable

```

## 参考文献
https://blog.csdn.net/wy250229163/article/details/52354278
