---
layout: post
title: LSTM
categories:
tags: 2C_NLP
keywords:
description:
order: 331
---


## 介绍

是一种RNN算法，Schmidhuber于1997年提出  


传统的RNN训练方法都有明显的缺点
- Gradient Descent和Back-Propagation Through Time(BPTT)
- Real-Time Recurrent Learning(RTRL)


时间间隔过长，这些算法都有梯度弥散问题

## 原理
1. 第一步与RNN一样
$i_t=\sigma(B_th_{t-1}+A_i x_t)$  
2. 记忆单元
$C_t=tanh(B_C h_{t-1}+A_C x_t)$
3. 通过忘记矩阵（forget matrix）修改可选的记忆单元
$f_t=\sigma(B_fh_{t-1}+A_f x_t)$
4. 结合上面的记忆信息和以往信息
$N_t=i_t C_t+f_t N_{t-1}$
5. 综合以上，进行输出
$O_t=\sigma(B_O h_{t-1}+A_O x_t+D_O N_t)$
6. 迭代更新
$h_t=O_t tanh(N_t)$





## 参考文献
《Matlab神经网络原理与实例精解》陈明，清华大学出版社   
《神经网络43个案例》王小川，北京航空航天大学出版社  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战》中国工信出版集团
