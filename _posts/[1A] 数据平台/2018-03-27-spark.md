---
layout: post
title: 【spark】数据读写.
categories:
tags: 1A_数据平台
keywords:
description:
order: 151
---



![enter image description here](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.png?raw=true)

[xmind文档](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.xmind)
## Spark与Hive

### Hive to Spark
```py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("appName").enableHiveSupport().getOrCreate()
spark.sql('select ...')
```
<!-- from pyspark.sql import HiveContext
hiveCtx=HiveContext(sc)
df_data=hiveCtx.sql('select ...') -->

### Spark to Hive

```py
df_data.registerTempTable('table2')
# df_data.toDF('col_name1','col_name2').registerTempTable('table3') #可以改字段名

# 插入普通表
spark.sql('create table tmp.tmp_test_pyspark2hive as select * from table2')

# 静态插入分区表
spark.sql('''
INSERT OVERWRITE TABLE tmp.tmp_test PARTITION(dt='2018-05-02')
select * from tmp_test
''')

# 动态插入分区表(需要先把partition.mode配置为nonstrict)
spark.conf.set("hive.exec.dynamic.partition", "true")
spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")
# 也可以在启用 SparkSession 时设置
# spark=SparkSession.builder.appName("tmp_test").\
#     config("hive.exec.dynamic.partition", "true").\
#     config("hive.exec.dynamic.partition.mode", "nonstrict").\
#     enableHiveSupport().getOrCreate()
spark.sql('''
INSERT OVERWRITE TABLE tmp.tmp_tmp_test PARTITION(a,b)
SELECT c,d,e,a,b
FROM tmp_test
''')
```

## Spark与pandas
### spark to pandas

```py
df_data.toPandas() #返回DataFrame类型
```
### pandas to spark
```py
spark.createDataFrame(<pd.DataFrame>)
```

## DataFrame 与 RDD
sql得到的数据都是DataFrame类型
```py
rdd1=df_data.rdd
# 试试这个RDD：
rdd1.map(lambda row:row.sku_id)

spark.createDataFrame(rdd1)
```




## spark硬盘文件
```py
<pd.DataFrame>.to_csv('') #pandas 存储到本地csv
os.system("hive -e 'select * from ...'>test.csv") #hive存储到本地csv
```

硬盘文件到hive
```sql
create table if not exists mytable(key INT,value STRING)
ROW FOWMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH 'test.csv' INTO TABLE mytable;
```

<!--
#### text
```
RDD1.saveAsTextFile('spark_output_txt.txt')
```
#### json
```
import json
data=input.map(lambda x:json.loads(x))
RDD.map(lambda x:json.dumps(x)).saveAsTextFile(outputFile)
```

### json to Spark
```py
input=hiveCtx.jsonFile(inputFile)
input.registerTempTable('table_name')
df_data=hiveCtx.sql('select * from table_name')
```
 -->




## 参考文献
https://blog.csdn.net/wy250229163/article/details/52354278  
https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html
