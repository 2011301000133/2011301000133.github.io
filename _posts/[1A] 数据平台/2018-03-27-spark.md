---
layout: post
title: spark用法速查.
categories:
tags: 1B_Pandas
keywords:
description:
order: 151
---


## 交互

![enter image description here](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.png?raw=true)

[xmind文档](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.xmind)
## Spark与Hive的交互

### Hive to Spark
```py
from pyspark.sql import HiveContext
hiveCtx=HiveContext(sc)
df_data=hiveCtx.sql('select ...')
```

### Spark to Hive

```py
df_data.registerTempTable('table2')
# df_data.toDF('col_name1','col_name2').registerTempTable('table3') #可以改字段名
hiveCtx.sql('create table tmp.tmp_test_pyspark2hive as select * from table2')
```

## Spark与pandas的交互
### spark to pandas

```py
df_data.toPandas() #返回DataFrame类型
```
### pandas to spark
```py
hiveCtx.createDataFrame(<pd.DataFrame>)
```

## DataFrame 与 RDD
sql得到的数据都是DataFrame类型
```py
rdd1=df_data.rdd
# 试试这个RDD：
rdd1.map(lambda row:row.sku_id)

hiveCtx.createDataFrame(rdd1)
```




## 与硬盘文件交互
```py
<pd.DataFrame>.to_csv('') #pandas 存储到本地csv
os.system("hive -e 'select * from ...'>test.csv") #hive存储到本地csv
```
sql:
```sql
create table if not exists mytable(key INT,value STRING)
ROW FOWMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH 'test.csv' INTO TABLE mytable;
```

<!--
#### text
```
RDD1.saveAsTextFile('spark_output_txt.txt')
```
#### json
```
import json
data=input.map(lambda x:json.loads(x))
RDD.map(lambda x:json.dumps(x)).saveAsTextFile(outputFile)
```

### json to Spark
```py
input=hiveCtx.jsonFile(inputFile)
input.registerTempTable('table_name')
df_data=hiveCtx.sql('select * from table_name')
```
 -->


## 教程
### 1. 初始化SparkContext
```py
from pyspark import SparkContext, SparkConf
conf=SparkConf().setMaster('local').setAppName('My App')#local可以改为local[*]，以增加核心数
sc=SparkContext(conf=conf)
```

### 2. 创建RDD
```py
lines=sc.parallelize(['pandas','i like pandas','hello','hello world!','error line','warning line'])
lines=sc.textFile('test.txt')
```


RDD有两种操作：**transformation**， **actions**
### 3. transformation


```py
RDD1.map(func)#func接受每一个元素，返回每一个元素，作为新RDD的元素
RDD1.filter(func)#筛选满足条件的RDD值，func接受每一个元素，返回一个bool类型
RDD1.flatMap(func) #如果func返回的是一个迭代器，那么把迭代器里所有对象放入新RDD中并摊平

#集合操作
RDD1.distinct()
RDD1.union(RDD2) #并集
RDD1.intersection(RDD2) #交集
RDD1.subtract(RDD2) #差集

RDD1.cartesian(RDD2) #笛卡尔积

RDD1.sample(withReplacement=False,fraction=0.2) #采样
```

例子：
```py
errorsRDD=lines.filter(lambda line:'error' in line)
warningRDD=lines.filter(lambda line:'warning' in line)

badRDD=errorsRDD.union(warningRDD)# 合并
```


### 4. actions

```py
RDD1.reduce(lambda x,y:x+y)
RDD1.fold(0,lambda x,y:x+y) #相当于一个带初始值的reduce

RDD1.foreach(func1) #对每个元素使用func1？？？
RDD1.aggregate(zeroValue,seqOp,combOp)# 没搞太清楚？？？

RDD1.collect()
RDD1.take(10)
RDD1.top(10)
RDD1.first()

RDD1.count()
RDD1.countByValue() #返回dict，内容是每个元素的个数
```



### pair RDD

pair RDD是一种特殊的RDD  
转化：  
```py
pair_RDD1=sc.parallelize([(1,2),(5,2),(3,4),(3,3)])
pair_RDD2=sc.parallelize([(1,5),(6,2)])

pair_RDD1.reduceByKey(func) #分组应用func,例如func=lambda x,y:x+y。是transformation(对比reduce是actions)
pari_RDD1.foldByKey(0，func) #类似fold，带初始值的reduce。是transformation，fold是action


pair_RDD1.groupByKey() # 同一个key下的value变成[value1,value2,...]？？？没明白
combineByKey#???


pair_RDD1.flatMapValues(func) #func 每次接受1个value，返回iterable。返回的RDD保留key，摊平value
pair_RDD1.mapValues(func) #对每个value应用func，key保持不变


pair_RDD1.keys()#返回key组成的RDD
pair_RDD1.values()#返回value组成的RDD
pair_RDD1.sortByKey()#按照key排序
# 按key排序的高级玩法
pair_RDD1.sortByKey(ascending=True,keyfunc=lambda x:str(x)) #ascending默认为True，keyfunc可以把原本元素转化成新对象，然后用新对象排序
```
双pair RDD操作：
```py

pair_RDD1.subtractByKey(pair_RDD2) #删除key相同的元素
pair_RDD1.join(pair_RDD2)  #内连接，返回类似[(1, (2, 5))]的RDD
pair_RDD1.leftOuterJoin(pair_RDD2) #左边的保留
pair_RDD1.rightOuterJoin(pair_RDD2) #右边的保留
pair_RDD1.cogroup(pair_RDD2) #key-value的RDD，其中value是一个iterable

```

pari RDD的行动：
```py
countByKey() #返回key的计数
lookup(key) #返回key所对应的所有value
```



## spark-submit
```py
command = \
    "spark-submit" \
    "    --master yarn" \
    "    --deploy-mode client" \
    "    --driver-memory 6g" \
    "    --executor-memory 10g" \
    "    --num-executors 100" \
    "    --executor-cores 6" \
    "    --conf spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class=DockerLinuxContainer" \
    "    --conf spark.executorEnv.yarn.nodemanager.container-executor.class=DockerLinuxContainer" \
    "    --conf spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name=bdp-docker.jd.com:5000/wise_algorithm:latest" \
    "    --conf spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name=bdp-docker.jd.com:5000/wise_algorithm:latest" \
    "    {pyfile} str1={str1} str2={str2} "

command = command.format(pyfile='pyfile.py',str1='str1',str2='str2')
print(command)
os.system(command)

# --master:
#      spark://host:port  独立集群作为集群url
#      yarn     yarn作为集群
#      local    本地模式
#      local[N] 本地模式，n个核心
#      local[*] 本地模式，最大核心

# --deploy-mode:
#     client本地，cluster集群

# --executor-memory 执行器的内存
# --driver-memory 驱动器的内存

```



```py
#.py文件，为了展示str1变量可以传入所提交的.py文件

def parseArgs(arrs=sys.argv,split_str='='):
    dict = {}
    for i in range(0, len(arrs)):
        if sys.argv[i].find('=') != -1:
            n, v = arrs[i].split('=')
            dict[n]=v
    return dict

dict=parseArgs()
dict['']

conf = SparkConf().setAppName('app_name')

sc = SparkContext(conf=conf)
hiveCtx = HiveContext(sc)

# 主程序
# 。。。
# 。。。

sc.stop()
```

shell中也可以使用类似的配置
```
spark-shell --master spark://masternode:7077
pyspark --master spark://masternode:7077
```



## 参考文献
https://blog.csdn.net/wy250229163/article/details/52354278
