---
layout: post
title: 【spark】数据格式互转.
categories:
tags: 1A_数据平台
keywords:
description:
order: 153
---



![enter image description here](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.png?raw=true)

[xmind文档](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/spark.xmind)
## Spark与Hive

### Hive to Spark
```py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("appName").enableHiveSupport().getOrCreate()
spark.sql('select ...')
```
<!-- from pyspark.sql import HiveContext
hiveCtx=HiveContext(sc)
df_data=hiveCtx.sql('select ...') -->

### Spark to Hive

```py
df_data.registerTempTable('table2')
# df_data.toDF('col_name1','col_name2').registerTempTable('table3') #可以改字段名

# 插入普通表
spark.sql('create table tmp.tmp_test_pyspark2hive as select * from table2')

# 静态插入分区表
spark.sql('''
INSERT OVERWRITE TABLE tmp.tmp_test PARTITION(dt='2018-05-02')
select * from tmp_test
''')

# 动态插入分区表(需要先把partition.mode配置为nonstrict)
spark.conf.set("hive.exec.dynamic.partition", "true")
spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")
# 也可以在启用 SparkSession 时设置
# spark=SparkSession.builder.appName("tmp_test").\
#     config("hive.exec.dynamic.partition", "true").\
#     config("hive.exec.dynamic.partition.mode", "nonstrict").\
#     enableHiveSupport().getOrCreate()
spark.sql('''
INSERT OVERWRITE TABLE tmp.tmp_tmp_test PARTITION(a,b)
SELECT c,d,e,a,b
FROM tmp_test
''')
```

## Spark与pandas
### spark to pandas

```py
df_data.toPandas() #返回DataFrame类型
```
### pandas to spark
```py
spark.createDataFrame(<pd.DataFrame>)
```

## DataFrame 与 RDD
sql得到的数据都是DataFrame类型
```py
rdd1=df_data.rdd
# 试试这个RDD：
rdd1.map(lambda row:row.sku_id)

spark.createDataFrame(rdd1)
```

## Spark与HDFS

```py
df.write.format('csv').save('hdfs://ns11/user/mart_rmb/guofei8/abc')
df_a = spark.read.format('csv').load('hdfs://ns11/user/mart_rmb/guofei8/abc')
# $ hadoop fs -ls hdfs://ns11/user/mart_rmb/guofei8/abc

df1_2.write.mode('overwrite').format('orc').partitionBy('dt').saveAsTable('tmp.tmp_test_guofei')
# `append`, `overwrite`, `errorifexists`, `ignore`
# 'overwrite' ：普通表:被覆盖，分区表所有分区被覆盖（相当于先清除数据，然后写入数据）
# 'append'  ： 无论是普通表还是分区表，原数据不会丢失，新数据也不会少
# 往往想要覆盖所涉及的分区，未涉及到的分区不变，两种方法： 1. 上面的sql语句去 overwrite  2. 先用命令drop相关分区，然后append
```

## spark硬盘文件
```py
<pd.DataFrame>.to_csv('') #pandas 存储到本地csv
os.system("hive -e 'select * from ...'>test.csv") #hive存储到本地csv
```

硬盘文件到hive
```sql
create table if not exists mytable(key INT,value STRING)
ROW FOWMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH 'test.csv' INTO TABLE mytable;
```

<!--
#### text
```
RDD1.saveAsTextFile('spark_output_txt.txt')
```
#### json
```
import json
data=input.map(lambda x:json.loads(x))
RDD.map(lambda x:json.dumps(x)).saveAsTextFile(outputFile)
```

### json to Spark
```py
input=hiveCtx.jsonFile(inputFile)
input.registerTempTable('table_name')
df_data=hiveCtx.sql('select * from table_name')
```
 -->


## hadoop hdfs常用命令
```bash
hadoop fs
# 查看Hadoop HDFS支持的所有命令

hadoop fs -ls # 列出目录及文件信息
hadoop fs -ls /user/abc # 列出指定目录信息
hadoop fs -lsr # 循环列出目录、子目录及文件信息


# 本地复制
hadoop fs -put test.txt /user/sunlightcs
# 将本地文件系统的test.txt复制到HDFS文件系统的/user/sunlightcs目录下
hadoop fs -get /user/sunlightcs/test.txt
# 将HDFS中的test.txt复制到本地文件系统中，与-put命令相反


# 查看内容
hadoop fs -cat /user/sunlightcs/test.txt
# 查看HDFS文件系统里test.txt的内容
hadoop fs -tail /user/sunlightcs/test.txt
# 查看最后1KB的内容


# 删除
hadoop fs -rm /user/sunlightcs/test.txt
# 从HDFS文件系统删除test.txt文件，rm命令也可以删除空目录
hadoop fs -rmr /user/sunlightcs
# 删除/user/sunlightcs目录以及所有子目录


# hadoop fs -chgrp [-R] /user/sunlightcs
# 修改HDFS系统中/user/sunlightcs目录所属群组，选项-R递归执行，跟linux命令一样
#
# hadoop fs -chown [-R] /user/sunlightcs
# 修改HDFS系统中/user/sunlightcs目录拥有者，选项-R递归执行
#
# hadoop fs -chmod [-R] MODE /user/sunlightcs
# 修改HDFS系统中/user/sunlightcs目录权限，MODE可以为相应权限的3位数或+/-{rwx}，选项-R递归执行
#
# hadoop fs -count [-q] PATH
# 查看PATH目录下，子目录数、文件数、文件大小、文件名/目录名
#
# hadoop fs -cp SRC [SRC …] DST      
# 将文件从SRC复制到DST，如果指定了多个SRC，则DST必须为一个目录
#
# hadoop fs -du PATH
# 显示该目录中每个文件或目录的大小
#
# hadoop fs -dus PATH
# 类似于du，PATH为目录时，会显示该目录的总大小
#
# hadoop fs -expunge
# 清空回收站，文件被删除时，它首先会移到临时目录.Trash/中，当超过延迟时间之后，文件才会被永久删除
#
# hadoop fs -getmerge SRC [SRC …] LOCALDST [addnl]     
# 获取由SRC指定的所有文件，将它们合并为单个文件，并写入本地文件系统中的LOCALDST，选项addnl将在每个文件的末尾处加上一个换行符
#
# hadoop fs -touchz PATH  
# 创建长度为0的空文件
#
# hadoop fs -test -[ezd] PATH    
# 对PATH进行如下类型的检查：
# -e PATH是否存在，如果PATH存在，返回0，否则返回1
# -z 文件是否为空，如果长度为0，返回0，否则返回1
# -d 是否为目录，如果PATH为目录，返回0，否则返回1
#
# hadoop fs -text PATH
# 显示文件的内容，当文件为文本文件时，等同于cat，文件为压缩格式（gzip以及hadoop的二进制序列文件格式）时，会先解压缩
#
# hadoop fs -help ls
# 查看某个[ls]命令的帮助文档
```

## yarn
```bash
yarn application -kill application_id # 杀掉一个 job
```


## 1. 初始化
```py
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("app_name").enableHiveSupport().getOrCreate()
```


## 2. spark-submit
```py
command1 = '''
spark-submit   --master yarn \
      --deploy-mode {deploymode} \
      --driver-memory 6g \
      --executor-memory 10g \
      --num-executors 100 \
      --executor-cores 6 \
      --conf spark.yarn.appMasterEnv.yarn.nodemanager.container-executor.class=DockerLinuxContainer \
      --conf spark.executorEnv.yarn.nodemanager.container-executor.class=DockerLinuxContainer \
      --conf spark.yarn.appMasterEnv.yarn.nodemanager.docker-container-executor.image-name=my-docker.guofei.me:5000/wise_algorithm:latest \
      --conf spark.executorEnv.yarn.nodemanager.docker-container-executor.image-name=my-docker.guofei.me:5000/wise_algorithm:latest \
      --py-files jieba.zip \
      {pyfile} cal_dt_str={cal_dt_str}
'''
# --master:
#      spark://host:port  独立集群作为集群url
#      yarn     yarn作为集群
#      local    本地模式
#      local[N] 本地模式，n个核心
#      local[*] 本地模式，最大核心

# --deploy-mode:
#     client本地，cluster集群

# --executor-memory 执行器的内存
# --driver-memory 驱动器的内存


import subprocess

cal_dt = datetime.datetime(year=2018, month=1, day=13)
end_dt= datetime.datetime(year=2017, month=12, day=1)
# cal_dt+=28*oneday

iter_num = (cal_dt-end_dt)/oneday
job_num = 5
for i in range(iter_num // job_num):
    subjob = list()
    for i in range(job_num):
        print('#' * 1000)
        print(cal_dt)
        print('#' * 1000)
        a = subprocess.Popen(command1.format(deploymode='cluster', pyfile='file_name.py',cal_dt_str=cal_dt.strftime('%Y-%m-%d')), shell=True)
        subjob.append(a)
        cal_dt -= 1 * oneday
    for i in subjob:
        i.wait()
```



```py
#.py文件，为了展示str1变量可以传入所提交的.py文件

def parseArgs(arrs=sys.argv,split_str='='):
    dict = {}
    for i in range(0, len(arrs)):
        if sys.argv[i].find('=') != -1:
            n, v = arrs[i].split('=')
            dict[n]=v
    return dict

dict=parseArgs()
dict['']

conf = SparkConf().setAppName('app_name')

sc = SparkContext(conf=conf)
hiveCtx = HiveContext(sc)

# 主程序
# 。。。
# 。。。

sc.stop()
```

启动shell时，也可以使用类似的配置
```bash
spark-shell --master spark://masternode:7077
pyspark --master spark://masternode:7077
```

## 其它实用技巧
```py
spark.sparkContext.uiWebUrl
# Spark Web UI
```
## 参考文献
https://blog.csdn.net/wy250229163/article/details/52354278  
https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html  
https://blog.csdn.net/gz747622641/article/details/54133728
