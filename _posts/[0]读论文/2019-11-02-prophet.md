---
layout: post
title: 【读论文】Distilling the Knowledge in a Neural Network
categories:
tags: 0_读论文
keywords:
description:
order: 1
---



- Distilling the knowledge in a neural network (2015), G. Hinton et al. [pdf](https://arxiv.org/pdf/1503.02531.pdf)
- 镜像地址 [pdf](/pictures_for_blog/papers/Understanding_Generalization_Transfer/Distilling%20the%20Knowledge%20in%20a%20Neural%20Network.pdf)

## abstract

ensemble 方法确实不错，但太消耗算力。这里提出一种 Distilling the Knowledge 的方法，使训练快速、并行。

## introduction

我们可以训练一个笨重的模型，然后用 Distilling the Knowledge 在部署阶段部署一个轻量的模型。

在很多个class的多分类模型中，有些class的概率很低，但objective function仍然要计算他们。
