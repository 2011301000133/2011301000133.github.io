---
layout: post
title: 【学习笔记】神经网络（1）.
categories:
tags: 2有监督学习
keywords:
description:
order: 250
---

## 前论

### 优点

1. 自学习和自适应  
2. 非线性：现实世界是一个非线性的复杂系统，人脑也是.  
3. 鲁棒性:局部损坏只会削弱神经网络，而不会产生灾难性错误  
4. 计算的并行性：每个神经元独立处理信息  
5. 存储的分布性：知识不是存储于某一处，而是分布在所有连接权值中  


### 按结构分类

- 前向网络
    - 单层前向
    - 单层感知机
    - 线性网络
    - 多层前向
    - 多层感知机
    - 径向基
- 反馈网络
    - Hopfield网络
    - Elman网络


### 按功能分类
- 有监督学习
    - BP，径向基，Hopfield
- 无监督学习
    - 自组织神经网络
    - 竞争神经网络


此外，还有：
Hebb学习规则
纠错学习规则（用的多）
随机学习规则（Boltzmann机）
竞争学习规则


## 神经元的结构

### Linear neurons

$y=b+\sum\limits_ix_iw_i$

### Binary Threshold neuros

$z=\sum\limits_ix_iw_i$

$y=
\begin{cases}
1&   &if z>=0\\\\  
0&   & otherwize
\end{cases}$



### Rctified Linear Neuros

$z=b+\sum\limits_ix_iw_i$

$y=
\begin{cases}
z&    &if z>0\\\\  
0&    &otherwise
\end{cases}$

### Sigmoid neurons

$z=b+\sum\limits_ix_iw_i$

$y=\dfrac{1}{1+e^{-z}}$

- S-function
- 优点是连续、可微，而且微分是$y(1-y)$
- Sigmoid函数还有一种是tanh(z)

### Stochastic binary neurons

$z=b+\sum\limits_i x_i w_i$  

$p(s=1)=\dfrac{1}{1+e^{-z}}$
- 这种神经元依概率输出0或1


## 网络结构

### Feed-forward neural networks

### Recurrent networks

- 同一个layer之间互相关联
- They can have very complicated dynamics, and this can make them very difficult to train.

### Symmetrically connected networks

- Hopfield：无hidden units
- Boltzmann machines:有hidden units

## 学习的过程

- 字母意义
  - n 表示training set中的第n个
  - i 表示某个神经元的weight vector 中第i个
  - j 表示第j个神经元

### delta rule：linear的情况

- 平方形式的残差  
$E= 0.5*\sum\limits_{n\in training}(t_n-y_n)^2$

- 微分
$\dfrac{\partial E}{\partial w_i}
=0.5*\sum\limits_n \frac{\partial y_n}{\partial w_i} \frac{dE_n}{dy_n}
=-\sum\limits_n x_{in}(t_n-y_n)$

- 改变weight
$\Delta w_i=-\varepsilon \dfrac{\partial E}{\partial w_i}$

### delta rule: sigmoid的情况

1. sigmoid的神经元是这样的
$z=b+\sum\limits_ix_iw_i$
$y=\dfrac{1}{1+e^{-z}}$

2. sigmoid神经元的特点
$\dfrac{dy}{dz}= y(1-y)$

3. sigmoid神经元的学习过程
$\dfrac{\partial E}{\partial w_i}=-\sum\limits_n x_{in}y_n(1-y_n)(t_n-y_n)$

### 误差向前传播

#### 变量定义

I表示中间层，J表示输出层
n表示第n个case/sample
v表示输出，u表示输入，故而v=g(u)，其中 g()是神经元内的函数

学习过程$w_{ij}(n+1)=\Delta w_{ij}(n)+w_{ij}(n)$

#### 对于第n个case,J层情况

$\dfrac{\partial e(n)}{\partial w_{ij}}
=\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}
\dfrac{\partial u_j}{\partial w_{ij}}$  
- 关于第一项  
$e=0.5\sum e_j^2$  
于是$\dfrac{\partial e}{\partial e_j}=e_j=t_j-v_j^J$  
- 关于第二项  
$e_j=t_j-v_j^J$  
于是$\dfrac{\partial e_j}{\partial v_j}=-1$  
- 关于第三项  
$v_j^J=g(u_j^J)$  
于是$\frac{\partial v_j}{\partial u_j}=g^1(u_j^J)$  
- 关于第四项  
$u_j^J=\sum\limits_{i\in I}w_{ij}v_i^I $  
于是$\dfrac{\partial u_j}{\partial w_{ij}}=v_i^I$  


#### 对于第n个case，J层学习

$\Delta w_{ij}(n)=- \eta \dfrac{\partial e(n)}{\partial w_{ij}}$

- 用链式法则拆分

$\Delta w_{ij}(n)=- \eta
\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}
v_i^I$

- 定义梯度为：

$\delta_j^J=
\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}
$

于是:
$\Delta w_{ij}(n)=-\eta \delta_j^J v_i^I$

#### 对于第n个case，I层学习

(I层之前为M层)
$\Delta w_{mi}^I= - \eta \dfrac{\partial e}{\partial w_{mi}}$

- 用链式法则拆分

$\dfrac{\partial e}{\partial w_{mi}}
=\dfrac{\partial e}{\partial v_i^I}
\dfrac{\partial v_i^I}{\partial u_i^I}
\dfrac{\partial u_i^I}{\partial w_{mi}}$


- 用梯度形式改写

$\Delta w_{mi}^I=-\eta \delta_i^I v_m^M$

- 其中梯度为：

$\delta_i^I=
\dfrac{\partial e}{\partial v_i}
\dfrac{\partial v_i}{\partial u_i}
$

经计算，其中，

$\dfrac{\partial e}{\partial v_i^I}=\sum\limits_{j\in J}\delta_j^Jw_{ij}$


### 关于Linear

纯linear 也服从误差向前传播，并且注意多层linear网络与单层linear网络没有区别（证明）

### 关于softmax output function

#### unit 的结构

$y_i=\dfrac{e^{z_i}}{\sum\limits_{j \in group}e^{z_j}}$  
于是  
$\dfrac{\partial y_i}{\partial z_i}=y_i(1-y_i)$  

#### 对应的cost function

这时，cost function 不应当是误差平方和了，而是这样  
$C=-\sum\limits_j t_j log y_j$  
此时$\dfrac{\partial C}{\partial z_i}=y_i-t_i$  

以二值为例：  
$y=\dfrac{1}{1+e^{-z}}$  
如果cost function是误差平方和$E=0.5(y-t)^2$  
那么$\dfrac{dE}{dz}=(y-t)y(1-y)$  
这时，如果y接近0或接近1，那么学习速度将会非常小  
合适的cost function是这样  
$E=-tlog(y)-(1-t)log(1-y)$,因为这时$\dfrac{dE}{dz}=y-t$  

模型总结：  
1、二值时，模型就是logistics回归  
2、多值时，似乎是最大熵模型  
