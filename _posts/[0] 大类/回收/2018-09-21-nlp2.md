---
layout: post
title: 【回收】【NLP2】TF-IDF
categories:
tags: 3_NLP
keywords:
description:
order: 301
---

## 介绍
TF-IDF(Text Frequency-Inverse Document Frequency)   
有时候我们想要调整某些特定单词的权重，提高有用单词的权重，降低常用或无意义单词的权重。  


$f_{ij}=$frequency of term (feature) i in doc j  
$TF_{ij}=\dfrac{f_{ij}}{\sum_k f_{kj}}$  


$n_i=$number of docs that mention term i  
$N=$total number of docs  
$IDF_{i}=\log\dfrac{N}{n_i}$  


$TF-IDF_{ij}=TF_{ij}\times IDF_i$  


```py
from sklearn.feature_extraction import text
tf_idf_transformer=text.TfidfTransformer()

counts = [[3, 0, 1],
           [2, 0, 0],
           [3, 0, 0],
           [4, 0, 0],
           [3, 2, 0],
           [3, 0, 2]]
tf_idf_transformer.fit(counts)

tf_idf_transformer.transform(counts).toarray()
```
由于tf-idf经常用于文本特征，因此有另一个类称为TfidfVectorizer，将 CountVectorizer 和 TfidfTransformer 的所有选项合并在一个模型中：
```py
TfidfVectorizer # =CountVectorizer+TfidfTransformer
```

## 参考资料
https://blog.csdn.net/pipisorry/article/details/41957763  
Nick McClure:《TensorFlow机器学习实战指南》 机械工业出版社  
lan Goodfellow:《深度学习》 人民邮电出版社  
王琛等：《深度学习原理与TensorFlow实战》 电子工业出版社  
李嘉璇：《TensorFlow技术解析与实战》 人民邮电出版社  
黄文坚：《TensorFlow实战》 电子工业出版社  
郑泽宇等：《TensorFlow实战Google深度学习框架》 电子工业出版社
