---
layout: post
title: 【SVM】理论与实现
categories: 算法
tags: 机器学习
keywords: model evaluation
description:
---

本文涉及以下内容
- SVM
- confusion matrix的实现
- 用pickle保存和加载模型

## SVM简介

SVM的优势是可以解决`小样本`、`非线性`、`高维模式识别`的问题  

SVM建立在`统计学习理论`中的`VC维理论`和`结构风险最小理论`的基础上，根据有限样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折中    
数据点是n维实空间的点，希望把这些点用一个n-1维的超平面分开。这类分类器叫做`线性分类器`  
有很多分类器符合这种要求。但是我们还希望找到最佳分割平面，也就是使得两个类的数据点间隔最大的那个平面。  

### 优缺点

优点   
- 泛化性高
- 小样本也可以
- 可以解决高维问题
- 可以解决非线性问题
- 鲁棒性：不需要微调
- 计算简单，理论完善：基于VC推广理论框架
- 与传统统计理论相比，统计学习理论基本`不涉及概率测度的定义和大数定律`
- 建立了有限样本学习问题的统一框架
- 避免了神经网络的网络结构选择、过学习、欠学习以及局部最小值问题


缺点  
- 对缺失数据敏感
- kernel 的选择

## 注意的问题
### 非线性问题

在解决非线性问题时，用kernel把数据映射到高维空间。（详见参考文献）  

kernel有以下几种：

1. 多项式核$k(x_1,x_2)=(<x_1,x_2>+R)^d$
2. 高斯核$k(x_1,x_2)=\exp(-\dfrac{||x_1-x_2||^2}{2\sigma^2})$  
高斯核把原始空间映射到无穷维空间，
    - $\sigma$很大时，高次特征衰减很快，近似相当于低维空间
    - $\sigma$很小时，可以将任意数据线性可分，但会带来严重的过拟合问题
3. 线性核$k(x_1,x_2)=<x_1,x_2>$，就是线性SVM，为了编程时形式上的统一

### outlier

如果support vector 里存在 outlier 的话，其影响就很大。  
可能使margin变小，甚至变成线性不可分问题。  
为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。

## Python示例

```py
from sklearn import datasets
dataset=datasets.load_iris()

from sklearn import svm
clf=svm.SVC()
clf.fit(dataset.data,dataset.target)
```

```py
clf.predict(dataset.data)#判断数据属于哪个类别
clf.score(dataset.data,dataset.target)#准确率
```



## Python 实现2

### 1. 生成数据

随机地生成数据

```py
import numpy as np
import pandas as pd
from scipy.stats import uniform
x1 = uniform.rvs(loc=0, scale=1, size=100)
x2 = uniform.rvs(loc=0, scale=2, size=100)
data = pd.DataFrame({'x1': x1, 'x2': x2})
data['y'] = (data['x1'] + 0.5 * data['x2'] < 1) * 1
```

### 2. 数据可视化
```py
import matplotlib.pyplot as plt
plt.plot(data[data.y == 1]['x1'], data[data.y == 1]['x2'], '.')
plt.plot(data[data.y == 0]['x1'], data[data.y == 0]['x2'], '.')
plt.show()
```

<img src='http://www.guofei.site/public/postimg/svm_1.png'>


### 3. 选取训练集和测试集


```py
from numpy.random import shuffle  # 引入随机函数
n=data.shape[0]
mac=np.arange(n)#掩码，用于随机打乱顺序
shuffle(mac)  # 随机打乱数据
data=data.loc[mac]
data_train = data.iloc[:int(0.8*n),:]  # 选取80%为训练数据
data_test = data.iloc[int(0.8*n):,:]  # 选取20%为测试数据

x_train = data_train.loc[:,['x1','x2']]
y_train = data_train.loc[:, 'y']#svm的y需要输入1darray
x_test = data_test.loc[:,['x1','x2']]
y_test = data_test.loc[:, ['y']]
```

这段代码不算太美丽，以后看看能不能改进


### 4. 模型计算

```py
from sklearn import svm
model = svm.SVC()
model.fit(x_train, y_train)
```

### 5. 混淆矩阵

```py
from sklearn import metrics

cm_train = metrics.confusion_matrix(y_train, model.predict(x_train))  # 训练样本的混淆矩阵
cm_test = metrics.confusion_matrix(y_test, model.predict(x_test))  # 测试样本的混淆矩阵
print(cm_train)
print(cm_test)
```

### 6. 模型保存


```py
import pickle
pickle.dump(model, open('svm.model', 'wb'))
# 以后可以通过下面语句重新加载模型：
# model = pickle.load(open('../tmp/svm.model', 'rb'))
```

下次加载后，也不需要sklearn这个包了  


## 一个多分类的案例

### 1. 随机生成数据并画图

```py
import numpy as np
import pandas as pd
from scipy.stats import uniform
import matplotlib.pyplot as plt

x1 = uniform.rvs(loc=0, scale=1, size=1000)
x2 = uniform.rvs(loc=0, scale=2, size=1000)
data = pd.DataFrame({'x1': x1, 'x2': x2})
data['y'] = 0
data.loc[data['x1'] + 0.5 * data['x2'] > 0.7, 'y'] = 1
data.loc[data['x1'] + 0.5 * data['x2'] > 1.3, 'y'] = 2
plt.plot(data[data.y == 0]['x1'], data[data.y == 0]['x2'], '.')
plt.plot(data[data.y == 1]['x1'], data[data.y == 1]['x2'], '.')
plt.plot(data[data.y == 2]['x1'], data[data.y == 2]['x2'], '.')
plt.show()
print(data['y'].value_counts())
```

<img src='http://www.guofei.site/public/postimg/svm_2.png'>

还有print的内容：
```py
1    520
2    266
0    214
```

显示这个的原因是，有可能有某个类数量很少，导致test或train中没有这个类。  
解决方法：
1. 从业务或数据角度去解决。  
2. 每个类分别取test和train



### 2. 画图，略过


### 3. 选取训练集和测试集

这里想保证每个类都有80%数据选入train，代码这样改：  

```py
from numpy.random import shuffle  # 引入随机函数
n=data.shape[0]
mac=np.arange(n)#掩码，用于随机打乱顺序
shuffle(mac)  # 随机打乱数据
data=data.loc[mac]

data_train=pd.DataFrame()
data_test=pd.DataFrame()
for i , t in data.groupby(data.loc[:,'y']):
    n_temp=t.shape[0]
    data_train=pd.concat([data_train,t.iloc[:int(0.8*n_temp)]])
    data_test=pd.concat([data_test,t.iloc[int(0.8*n_temp):]])
```

这里用了几个技巧，你可以想想自己能不能写出来

### 4. 模型计算

```py
from sklearn import svm
model = svm.SVC()
model.fit(data_train.loc[:,['x1','x2']], data_train.loc[:,'y'])
```

### 5. 混淆矩阵

```py
from sklearn import metrics

cm_train = metrics.confusion_matrix(data_train.loc[:,'y'], model.predict(data_train.loc[:,['x1','x2']]))  # 训练样本的混淆矩阵
cm_test = metrics.confusion_matrix(data_test.loc[:,'y'], model.predict(data_test.loc[:,['x1','x2']]))  # 测试样本的混淆矩阵
print(cm_train)
print(cm_test)
```

### 6. 模型保存

没变化


### 7. 模型应用

```py
model.predict(data_test.loc[:,['x1','x2']])#预测

model.support_vectors_# get support vectors
model.support_# get indices of support vectors
model.n_support_# get number of support vectors for each class
```


```py
model.decision_function_shape='ovo'
dec = model.decision_function([[0.6,1]])
```
如果有n个类，那么上面的dec有n(n-1)/2列  

```py
model.decision_function_shape='ovr'
dec = model.decision_function([[0.6,1]])
```
dec有n列



## 参考文献：

[支持向量机通俗导论（理解SVM的三层境界）
](http://blog.csdn.net/v_july_v/article/details/7624837)

[sklearn官方说明-SVM](http://scikit-learn.org/stable/modules/svm.html)
