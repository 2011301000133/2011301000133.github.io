SVM的优势是可以解决`小样本`、`非线性`、`高维模式识别`的问题  

SVM建立在`统计学习理论`中的`VC维理论`和`结构风险最小理论`的基础上，根据有限样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折中    
数据点是n维实空间的点，希望把这些点用一个n-1维的超平面分开。这类分类器叫做`线性分类器`  
有很多分类器符合这种要求。但是我们还希望找到最佳分割平面，也就是使得两个类的数据点间隔最大的那个平面。  

### 优点
- 通用性：能够各种函数集中构造函数
- 鲁棒性：不需要微调
- 有效性：解决实际问题最好的方法
- 计算简单，理论完善：基于VC推广理论框架
- 与传统统计理论相比，统计学习理论基本`不涉及概率测度的定义和大数定律`
- 建立了有限样本学习问题的统一框架
- 避免了神经网络的网络结构选择、过学习、欠学习以及局部最小值问题


## Python 实现


### 1. 生成数据

生成随机数据：

```py
import numpy as np
import pandas as pd
from scipy.stats import uniform
import matplotlib.pyplot as plt

x1 = uniform.rvs(loc=0, scale=1, size=100)
x2 = uniform.rvs(loc=0, scale=2, size=100)
data = pd.DataFrame({'x1': x1, 'x2': x2})
data['y'] = (data['x1'] + 0.5 * data['x2'] < 1) * 1
plt.plot(data[data.y == 1]['x1'], data[data.y == 1]['x2'], '.')
plt.plot(data[data.y == 0]['x1'], data[data.y == 0]['x2'], '.')
plt.show()
```

<img src='http://www.guofei.site/public/postimg/svm_1.png'>

参考文献：

[支持向量机通俗导论（理解SVM的三层境界）
](http://blog.csdn.net/v_july_v/article/details/7624837)
