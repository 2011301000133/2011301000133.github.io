---
layout: post
title: 【AdaBoost】理论与实现
categories: 算法
tags: 机器学习
keywords: model evaluation
description:
---


## 组合算法的简介

通过聚合 **多个分类器** 的预测来提高分类准确率。
组合方法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行 **投票来进行分类** 。

但是： **组合方法比任意单分类器的效果要好吗？**

### 打个比方

考虑25个分类器的组合，其中每个分类器的误差均为0.35，并且 **相互独立** 。  

那么组合算法的误差是：  

$E=\sum\limits_{i=13}^{25} C_25^i e^i (1-e)^{25-i}=0.06$  

### 结论

上面这个例子说明，当满足以下条件时，组合分类器优于单个分类器
1. 基分类器之间 **相互独立**
2. 基分类器 **好于随机猜测分类器**  

## 组合算法的分类

很难保证基分类器相互独立，基分类器轻微相关的情况下，组合方法也可以提高分类的准确率，  

如何减少相关关系呢？  

这里提供4种方法  

### 1. 通过处理训练数据集

根据某种抽样分布，通过对原始数据进行再抽样来得到多个训练集，然后使用特定的学习算法为每一个训练集建立一个分类器。

典型：  
**装袋（bagging）** 和 **提升（boosting）**

### 2. 通过处理输入特征
通过选择输入特征的子集来形成每个训练集，子集可以随机选择，也可以根据领域专家的建议选择。适合含有大量冗余特征的数据集。  

典型：  
**随机森林（Random forest）**  

### 3. 通过处理类标号

通过将类标号随机划分成两个不相交的子集，把训练数据变换为二类问题，然后重新标记过的数据训练一个基分类器，重复多次，得到一组基分类器。  

典型：  

**错误-纠正输出编码（error-correcting output coding）**

### 4. 通过处理学习算法

在同一个训练数据集上多次执行算法可能得到不同的模型。

如：改变 **神经网络** 拓扑结构或各个神经元之间的连接的权重，就可以得到不同的模型。



**前三种属于一般性方法，适用于任何分类器，第四种方法依赖于使用的分类器类型。**  
