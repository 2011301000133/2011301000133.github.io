---
layout: post
title: 【tf.keras】笔记
categories:
tags: 2_3_神经网络与TF
keywords:
description:
order: 286
---

## 通用代码

导入包，载入数据
```python
import tensorflow as tf
from tensorflow import keras

fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
x_valid, y_valid = train_images[:5000], train_labels[:5000]
x_train, y_train = train_images[5000:], train_labels[5000:]

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
```

构建模型，运行模型
```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])
# 也可以用 model.add(keras.layers.Dense(128, activation='relu')) 来一步一步添加

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x=x_train, y=y_train, epochs=10, validation_data=(x_valid, y_valid))
# 或者 validation_split = 0.01 也可以自动分验证集

predictions = model.predict(test_images)
```

#### 显示模型的一些情况
```python
model.layers
model.summary()

history.history # 存放的是迭代过程中的一些值

model.evaluate(test_images, test_labels)
```

这里自编一个显示指标走势的代码


```python
import matplotlib.pyplot as plt

fig,ax=plt.subplots(1,1)

for i,j in history.history.items():
    ax.plot(j,label=i)

ax.set_ylim(0,1)
ax.legend()

plt.show()
```




### callbacks
- EarlyStopping
- ModelCheckpoint
- TensorBoard


```python
import os
logdir='.\\callbacks'
output_model_file=os.path.join(logdir,'fashin_mnist_model.h5')
callbacks=[keras.callbacks.TensorBoard(logdir),
           keras.callbacks.ModelCheckpoint(output_model_file,save_best_only=True), # 如果是 False，保存最后一个
           keras.callbacks.EarlyStopping(min_delta=1e-3,patience=5)

]


history = model.fit(x=train_images, y=train_labels, epochs=100, validation_split = 0.01 ,callbacks=callbacks)
```

TensorBoard：（跑不通？？？）
```bash
tensorboard --logdir=callbacks
```

#### EarlyStopping

```python
keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
```

### 归一化

代码待补，因为下面的 batch normalization，在数据归一化的前提下，效果才更好。

### batch normalization



批归一化可以缓解深度神经网络的梯度消失，因为每一层更加规整。

批归一化有3种做法：（下面代码中）
1. 先归一化再激活，
2. 先激活后归一化，
3. 用selu作为激活函数，这是个自带Normalization的激活函数，而且相比之下，训练速度快，训练效果好

```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28))
])
for _ in range(20):
    model.add(keras.layers.Dense(100,activation='selu'))

    # 先激活，后归一化
    #     model.add(keras.layers.Dense(100,activation='relu'))
    #     model.add(keras.layers.BatchNormalization())

    # 先归一化，再激活
    # model.add(keras.layers.Dense(100))
    # model.add(keras.layers.BatchNormalization())
    # model.add(keras.layers.Activation('relu'))

model.add(keras.layers.Dense(10, activation='softmax'))
```


### dropout

一般不会给每一层都添加dropout，而是最后几层。
```python
model.add(keras.layers.Dropout(rate=0.5))
model.add(keras.layers.AlphaDropout(rate=0.5))
```

关于 AlphaDropout：
- 均值和方差不变
- 因此归一化性质不变
- 因此可以和 BatchNormalization 连用


## Wide&Deep模型
Google 在 2016 年发布的模型，可以用于分类和回归问题。用于 Google Play 的推荐算法。

### 稀疏特征与密集特征
稀疏特征
- 是什么
  - 离散值特征
  - One-hot 编码可以产生
- 优缺点
  - 优点：工业界广泛使用
  - 缺点：需要人工设计。可能过拟合。叉乘后所有的可能太多了。

密集特征
- 是什么：实数向量表达
- 优缺点
  - 优点：距离等带有信息。兼容没有出现的特征组合。更少的人工参与。


step1：载入数据
```python
import tensorflow as tf
from tensorflow import keras
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

housing = fetch_california_housing()
x_train_all, x_test, y_train_all, y_test = train_test_split(
    housing.data, housing.target, random_state = 7)
x_train, x_valid, y_train, y_valid = train_test_split(
    x_train_all, y_train_all, random_state = 11)
```

step2:标准化
```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_valid_scaled = scaler.transform(x_valid)
x_test_scaled = scaler.transform(x_test)
```

step3：做模型
```python
input = keras.layers.Input(shape=x_train.shape[1:])
hidden1 = keras.layers.Dense(30, activation='relu')(input)
hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)
# 复合函数: f(x) = h(g(x))

concat = keras.layers.concatenate([input, hidden2])
output = keras.layers.Dense(1)(concat)

model = keras.models.Model(inputs = [input],
                           outputs = [output])


model.compile(optimizer='adam',
              loss='mean_squared_error')

history = model.fit(x=x_train_scaled, y=y_train, epochs=100, validation_data=(x_valid_scaled, y_valid))
# 或者 validation_split = 0.01 也可以自动分验证集

predictions = model.predict(x_test)
```

## 一个回归的例子
导入包和数据，拆分训练集
```python
from tensorflow import keras
import tensorflow as tf
from sklearn import model_selection
from sklearn import datasets

X, y, coef = \
    datasets.make_regression(n_samples=10000,
                             n_features=5,
                             n_informative=3,  # 其中，3个feature是有信息的
                             n_targets=1,  # 多少个 target
                             bias=1,  # 就是 intercept
                             coef=True,  # 为True时，会返回真实的coef值
                             noise=1,  # 噪声的标准差
                             )

X_train_all, X_test, y_train_all, y_test=model_selection.train_test_split(X,y,test_size=0.3)
X_train, X_valid, y_train, y_valid=model_selection.train_test_split(X_train_all,y_train_all,test_size=0.1)

```

建立模型
```python
model=keras.Sequential()
model.add(keras.layers.Dense(64,activation='relu',input_shape=X_train.shape[1:]))
model.add(keras.layers.Dense(64,activation='relu'))
model.add(keras.layers.Dense(1))

model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),
             loss='mse',
             metrics=['mae', 'mse'])
```

训练
```python
history=model.fit(x=X_train,y=y_train,epochs=100,validation_data=(X_valid,y_valid),
                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)])
```
