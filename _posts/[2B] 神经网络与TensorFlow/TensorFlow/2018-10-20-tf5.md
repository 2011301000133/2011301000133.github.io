---
layout: post
title: 【TensorFlow5】优化器
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 285
---


![optimization1](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/optimization/optimization1.gif?raw=true)
![optimization2](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/optimization/optimization2.gif?raw=true)

## 优化器

[官网解释1](https://www.tensorflow.org/api_docs/python/tf/train)  
[官网解释2](https://www.tensorflow.org/api_guides/python/train)  


```py
tf.train.GradientDescentOptimizer # 如果每次只传入batch，那么就变成随机梯度下降
tf.train.AdadeltaOptimizer

# 下面两个是常用的高阶Optimizer
tf.train.MomentumOptimizer # 学习率不只是考虑这一次的学习趋势，而且考虑上一次的学习趋势
tf.train.AdamOptimizer

tf.train.RMSPropOptimizer # alpha go所使用的优化器
```


## 优化器详解
### BGD
batch gradient descent, 就是输入全部数据求得平均误差，然后以此为依据，进行一次迭代更新

### SGD
stochastic gradient descent, 将数据分为很多batch，每次随机抽取一个batch进行迭代运算。  
缺点：
1. 由于是随机抽取，梯度不可避免会有误差，因此需要调整学习率。
2. 容易局部最优，容易陷入马鞍点


### Momentum法
模拟物理中的动量概念，优点是前期能够加速震荡，后期再局部最小值附近震荡时，能够抑制震荡，加快收敛。

### Nesterov Momentum法

### Adagrad法
自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。

### Adalelta
Adagrad法存在的问题：学习率单调递减，需要手动设置全局初始学习率。  
Adadelta法用一阶方法，近似模拟二阶牛顿法，解决了以上问题
### RMSProp法
与Momentum类似地方法，通过引入衰减系数，使每回合都衰减一定的比例。  
适用于RNN

### Adam
名称来源于 adaptive moment estimation. 根据损失函数对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。  


## 学习率
`tf.train.exponential_decay` 指数下降法减小学习率  
decayed_learning_rate=learning_rate * decay_rate ** (global_step/decay_steps)
```py
learning_rate=tf.train.exponential_decay(0.01,global_step=10000,decay_steps=100,decay_rate=0.96,staircase=True)
```
staircase=True时，(global_step/decay_steps)会转化成整数，使得学习率阶梯下降

学习率的图：
```py
import tensorflow as tf
sess=tf.Session()

global_step_=tf.constant(0)
c=tf.train.exponential_decay(learning_rate=0.01,global_step=global_step_,decay_steps=100,decay_rate=0.96,staircase=False)
d=tf.train.exponential_decay(learning_rate=0.01,global_step=global_step_,decay_steps=100,decay_rate=0.96,staircase=True)
steps,learning_rates_c,learning_rates_d=[],[],[]
for i in range(1000):
    steps.append(i)
    learning_rates_c.append(sess.run(c,feed_dict={global_step_:i}))
    learning_rates_d.append(sess.run(d,feed_dict={global_step_:i}))


import matplotlib.pyplot as plt
plt.plot(steps,learning_rates_c)
plt.plot(steps,learning_rates_d)
plt.show()
```
![learning_rate1](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/tf/learning_rate1.png?raw=true)



用法案例
```py
import tensorflow as tf
TRAINING_STEPS = 100
global_step = tf.Variable(0)
LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)

x = tf.Variable(tf.constant(5, dtype=tf.float32), name="x")
y = tf.square(x)
train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(TRAINING_STEPS):
        sess.run(train_op)
        if i % 10 == 0:
            LEARNING_RATE_value = sess.run(LEARNING_RATE)
            x_value = sess.run(x)
            print ("After %s iteration(s): x%s is %f, learning rate is %f."% (i+1, i+1, x_value, LEARNING_RATE_value))
```

## 参考文献
[TF官网-tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)  
