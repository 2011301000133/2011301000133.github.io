---
layout: post
title: 【TensorFlow2】运算符，激活函数，优化器
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 282
---

## 运算符
### 标量运算
add,subtract,multiply,div,  
exp,log,  
greater,less,equal  


以下标量运算支持 一维变量、多维矩阵、三种数据结构，自带broadcasting
```py
import tensorflow as tf
sess=tf.Session()
ones=tf.fill(dims=[2,3],value=1.0) # 如果value=1，返回int格式，TF的int格式和float格式不能直接运算
twos=tf.fill(dims=[2,3],value=2.0)


sess.run(tf.add(one2,twos))
sess.run(tf.subtract(ones,twos))
sess.run(tf.multiply(ones,twos))
sess.run(tf.div(ones,twos))

sess.run(tf.exp(twos)) # 返回e^x
sess.run(tf.log(twos)) # 返回 log_e x

# 比较运算符，返回bool类型，自带 broadcasting
sess.run(tf.greater(ones,twos)) # 注意上面写的，自带 broadcasting
sess.run(tf.less(ones,3))
sess.run(tf.equal(ones,1))
```

### 向量运算
concat, slice, split, shape, random_shuffle
```py
import tensorflow as tf
sess=tf.Session()
ones=tf.fill(dims=[2,3],value=1.0)
twos=tf.fill(dims=[2,3],value=2.0)


sess.run(tf.concat(values=[ones,twos],axis=0)) # 0是纵向拼接，1是横向拼接


const=tf.constant(np.arange(50).reshape(5,-1))
sess.run(tf.slice(const,begin=[1,1],size=[2,5]))
# 取出一个分块矩阵，
# 分块矩阵的左上角坐标是 begin （从0开始数起）
# 分块矩阵的各边长度是 size （size从1开始数起，-1表示取到底）
sess.run(const[2:4,:-1]) # a more pythonic way ， 与list类似，含头不含尾


split0,split1,split2=tf.split(value=const,num_or_size_splits=[3,4,3],axis=1)
# 功能：分割矩阵。示例代码是把矩阵分割成3列，4列，3列
# 或者这样写：
split0,split1,split2=sess.run(tf.split(value=const,num_or_size_splits=[3,4,3],axis=1))

sess.run(tf.random_shuffle(const)) # 打乱顺序，只是按列打乱

const.shape # 返回类似 TensorShape([Dimension(5), Dimension(10)]) 的对象
```



### 矩阵运算
MatMul, MatrixInvers,MatrixDeterminant
```py
import tensorflow as tf
sess=tf.Session()
const1=tf.truncated_normal(shape=(5,5),stddev=1.0)
const2=tf.truncated_normal(shape=(5,4),stddev=0.5)


sess.run(tf.matmul(const1,const2)) # 矩阵积
sess.run(tf.matrix_inverse(const1)) # 矩阵逆
sess.run(tf.matrix_determinant(const1)) # 方阵对应的行列式
```
### 带状态运算
assign, assign_add, assign_sub
```py
import tensorflow as tf
sess=tf.Session()
variable1=tf.Variable(initial_value=np.arange(25).reshape(5,-1),dtype=tf.float32)
ones=tf.fill(dims=(5,5),value=1.0)

new_variable1=tf.add(variable1,ones)
updater=tf.assign(variable1,new_variable1)
sess.run(tf.global_variables_initializer())

for i in range(5):
    sess.run(updater)
    print(sess.run(variable1))


# assign_add, assign_sub
import tensorflow as tf
sess=tf.Session()
variable1=tf.Variable(initial_value=np.arange(25).reshape(5,-1),dtype=tf.float32)
ones=tf.fill(dims=(5,5),value=1.0)


updater=tf.assign_add(variable1,ones)
sess.run(tf.global_variables_initializer())

for i in range(5):
    sess.run(updater)
    print(sess.run(variable1))
```


### 矩阵求和运算
reduce_sum, reduce_mean,reduce_prod, reduce_max, reduce_min,  
reduce_all, reduce_any
```py
import tensorflow as tf
import numpy as np
sess=tf.Session()
const1=tf.constant(np.arange(12).reshape(3,-1),dtype=tf.float32)

tf.reduce_sum(const1,axis=None,keep_dims=False) # 对行列求和，返回 66
# axis=None （default） 对所有元素求和
# axis=0 对行求和
# axis=1 对列求和
# keepdims=True 保持维度。例如，axis=1时，默认会把列求和后的结果压成1维，keepdims=True 保证结果的维度和 input_tensor 保持一致

# 一下同类函数，参数都类似于上面
reduce_mean,reduce_prod, reduce_max, reduce_min,

tf.reduce_all(input_tensor=const1>5,axis=1,keepdims=True)
tf.reduce_any(input_tensor=const1>5,axis=1,keepdims=True)
```

## 激活函数
### 神经网络组件
```py
tf.nn.relu
tf.sigmoid
tf.tanh

# 举例：
# a=tf.nn.relu(tf.matmul(x,w1)+biases1)
# y=tf.nn.relu(tf.matmul(a,w2)+biases2)
```
SoftMax，Sigmoid，ReLU，Convolution2D，MaxPooling

### dropout
用来减轻overfitting
```py
keep_prob=tf.placeholder(tf.float32) # 训练时小于1，预测时等于1，所以使用placeholder
hidden1_drop=tf.nn.dropout(hidden1,keep_prob)
```

### CNN相关

```py
tf.nn.conv2d(x,kernel,strides=[1,1,1,1],padding='SAME')
#strides[0,1]数表示卷积核的尺寸，
#stride[2]表示channel数量，灰度图片是1，RGB图是3
#stride[3]表示卷积核的数量
#padding='SAME'表示边界加上padding，使得卷积的输入和输出保持同样的尺寸

tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')#横竖步长为2，

```

```py
tf.nn.lrn(pool1,4,bias=1,alpha=0.001/9.0,beta=0.75)
#LRN模仿生物的侧抑制机制，对局部神经元创建竞争环境，使其中响应大的值相对更大，并抑制其它反馈小的神经元。从而增强泛化能力
#可以用于Pooling之后，也可以用于conv之后、Pooling之前
#适用于ReLu这种没有上界的激活函数，不适合Sigmoid这种有固定边界，或者能抑制过大值得激活函数
```




### 保存
Save，Restore
### 队列和同步运算
Enqueue，Deq
ueue，MutexAcquire，MuterRelease
### 控制流
Merge，Switch，Enter，Leave，NextIteration

```py
x=tf.placeholder(shape=(None,6),dtype=tf.float32)
w1=tf.Variable(tf.random_normal(shape=(6,2)))
a=tf.matmul(x,w1) #矩阵乘法

sess.run(tf.global_variables_initializer())
sess.run(a,feed_dict={x:rv.rvs(size=(3,6))})
```























## 参考文献
《Matlab神经网络原理与实例精解》陈明，清华大学出版社   
《神经网络43个案例》王小川，北京航空航天大学出版社  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社
《TensorFlow实战Google深度学习框架》，郑泽宇，中国工信出版集团
