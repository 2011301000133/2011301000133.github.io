---
layout: post
title: 【TensorFlow3】激活函数
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 283
---

## 激活函数
[更详细的内容见于TF官网](https://www.tensorflow.org/api_docs/python/tf/nn)  

### 激活函数
- ReLU  
ReLU(Rectifier Linear Unit),其函数为$max(0,x)$  
```py
tf.nn.relu
```
- ReLU6  
是 hard-sigmoid 的变种$min(max(0,x),6)$  
```py
tf.nn.relu6
```
- sigmoid
$1/(1+exp(-x))$
```py
tf.nn.sigmoid
```
- tanh
$\dfrac{1-e^{2x}}{1+e^{-2x}}$
```py
tf.nn.tanh
```
- softsign  
是符号函数的连续估计$x/(abs(x)+1)$  
```py
tf.nn.softsign
```
- softplus  
是ReLU的平滑版 $log(exp(x)+1)$
```py
tf.nn.softplus
```
- ELU(Exponential Linear Unit)  
类似于softplus  
```py
# 表达式为 (exp(x)+1) if x<0 else x
tf.nn.elu
```
|激励函数|优点|缺点|
|--|--|--|
|Sigmoid|不容易出现极端值|收敛速度慢
|ReLU|收敛速度快|容易出现极端值|

### 卷积函数

```py
tf.nn.conv2d(x,kernel,strides=[1,1,1,1],padding='SAME')
#strides[0,1]数表示卷积核的尺寸，
#stride[2]表示channel数量，灰度图片是1，RGB图是3
#stride[3]表示卷积核的数量
#padding='SAME'表示边界加上padding，使得卷积的输入和输出保持同样的尺寸

tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')#横竖步长为2，

```

### 神经网络组件
```py
tf.nn.relu
tf.sigmoid
tf.tanh

# 举例：
# a=tf.nn.relu(tf.matmul(x,w1)+biases1)
# y=tf.nn.relu(tf.matmul(a,w2)+biases2)
```
SoftMax，Sigmoid，ReLU，Convolution2D，MaxPooling

### dropout
用来减轻overfitting
```py
keep_prob=tf.placeholder(tf.float32) # 训练时小于1，预测时等于1，所以使用placeholder
hidden1_drop=tf.nn.dropout(hidden1,keep_prob)
```



```py
tf.nn.lrn(pool1,4,bias=1,alpha=0.001/9.0,beta=0.75)
#LRN模仿生物的侧抑制机制，对局部神经元创建竞争环境，使其中响应大的值相对更大，并抑制其它反馈小的神经元。从而增强泛化能力
#可以用于Pooling之后，也可以用于conv之后、Pooling之前
#适用于ReLu这种没有上界的激活函数，不适合Sigmoid这种有固定边界，或者能抑制过大值得激活函数
```


## 参考文献
[TF官网-tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战Google深度学习框架》，郑泽宇，中国工信出版集团  
