---
layout: post
title: 【TensorFlow3】激活函数，优化器
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 283
---

## 激活函数
[更详细的内容见于TF官网](https://www.tensorflow.org/api_docs/python/tf/nn)  

- ReLU  
ReLU(Rectifier Linear Unit),其函数为$max(0,x)$  
```py
tf.nn.relu
```
- ReLU6  
是 hard-sigmoid 的变种$min(max(0,x),6)$  
```py
tf.nn.relu6
```
- sigmoid
$1/(1+exp(-x))$
```py
tf.nn.sigmoid
```
- tanh
```py
tf.nn.tanh
```
- softsign  
是符号函数的连续估计$x/(abs(x)+1)$  
```py
tf.nn.softsign
```
- softplus  
是ReLU的平滑版 $log(exp(x)+1)$
```py
tf.nn.softplus
```
- ELU(Exponential Linear Unit)  
类似于softplus  
```py
# 表达式为 (exp(x)+1) if x<0 else x
tf.nn.elu
```



### 神经网络组件
```py
tf.nn.relu
tf.sigmoid
tf.tanh

# 举例：
# a=tf.nn.relu(tf.matmul(x,w1)+biases1)
# y=tf.nn.relu(tf.matmul(a,w2)+biases2)
```
SoftMax，Sigmoid，ReLU，Convolution2D，MaxPooling

### dropout
用来减轻overfitting
```py
keep_prob=tf.placeholder(tf.float32) # 训练时小于1，预测时等于1，所以使用placeholder
hidden1_drop=tf.nn.dropout(hidden1,keep_prob)
```

### CNN相关

```py
tf.nn.conv2d(x,kernel,strides=[1,1,1,1],padding='SAME')
#strides[0,1]数表示卷积核的尺寸，
#stride[2]表示channel数量，灰度图片是1，RGB图是3
#stride[3]表示卷积核的数量
#padding='SAME'表示边界加上padding，使得卷积的输入和输出保持同样的尺寸

tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')#横竖步长为2，

```

```py
tf.nn.lrn(pool1,4,bias=1,alpha=0.001/9.0,beta=0.75)
#LRN模仿生物的侧抑制机制，对局部神经元创建竞争环境，使其中响应大的值相对更大，并抑制其它反馈小的神经元。从而增强泛化能力
#可以用于Pooling之后，也可以用于conv之后、Pooling之前
#适用于ReLu这种没有上界的激活函数，不适合Sigmoid这种有固定边界，或者能抑制过大值得激活函数
```



## 损失函数
#### 1.交叉熵
$H(p,q)=-\sum p(x)\log q(x)$

```py
cross_entropy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))
# y_代表正确结果，y代表预测结果
# tf.clip_by_value可以把张量中的数值限制在某个范围内,这里为了防止log0报错
# reduce_mean:大概是求均值？
```

TensorFlow对softmax+crossentropy进行了封装
```py
cross_entropy=tf.nn.softmax_cross_entropy_with_logits(y,y_)
```

#### 2.MSE
$MSE(y,y')=\dfrac{\sum (y-y')^2}{n}$
```py
mse=tf.reduce_mean(tf.square(y_-y))
```

#### 3.自定义
例如，预测销量时，多预测一个损失1元，少预测1个损失10元。  
$Loss(y,y')=\sum f(y_i,y_i')$,  
其中，$$f(x,y)=\left\{\begin{array}{ccc}a(x-y)&x>y\\
b(y-x)&x\leq y\end{array}\right.$$

```py
loss=tf.reduce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))
```





## 优化器

[官网解释1](https://www.tensorflow.org/api_docs/python/tf/train)  
[官网解释2](https://www.tensorflow.org/api_guides/python/train)  


```py
tf.train.GradientDescentOptimizer # 如果每次只传入batch，那么就变成随机梯度下降
tf.train.AdadeltaOptimizer

# 下面两个是常用的高阶Optimizer
tf.train.MomentumOptimizer # 学习率不只是考虑这一次的学习趋势，而且考虑上一次的学习趋势
tf.train.AdamOptimizer

tf.train.RMSPropOptimizer # alpha go所使用的优化器
```



### 保存
Save，Restore
### 队列和同步运算
Enqueue，Deq
ueue，MutexAcquire，MuterRelease
### 控制流
Merge，Switch，Enter，Leave，NextIteration

```py
x=tf.placeholder(shape=(None,6),dtype=tf.float32)
w1=tf.Variable(tf.random_normal(shape=(6,2)))
a=tf.matmul(x,w1) #矩阵乘法

sess.run(tf.global_variables_initializer())
sess.run(a,feed_dict={x:rv.rvs(size=(3,6))})
```



## 学习率
`tf.train.exponential_decay` 指数下降法减小学习率  
decayed_learning_rate=learning_rate * decay_rate ** (global_step/decay_steps)
```py
learning_rate=tf.train.exponential_decay(0.01,global_step=10000,decay_steps=100,decay_rate=0.96,staircase=True)
```
staircase=True时，(global_step/decay_steps)会转化成整数，使得学习率阶梯下降

学习率的图：
```py
import tensorflow as tf
sess=tf.Session()

global_step_=tf.constant(0)
c=tf.train.exponential_decay(learning_rate=0.01,global_step=global_step_,decay_steps=100,decay_rate=0.96,staircase=False)
d=tf.train.exponential_decay(learning_rate=0.01,global_step=global_step_,decay_steps=100,decay_rate=0.96,staircase=True)
steps,learning_rates_c,learning_rates_d=[],[],[]
for i in range(1000):
    steps.append(i)
    learning_rates_c.append(sess.run(c,feed_dict={global_step_:i}))
    learning_rates_d.append(sess.run(d,feed_dict={global_step_:i}))


import matplotlib.pyplot as plt
plt.plot(steps,learning_rates_c)
plt.plot(steps,learning_rates_d)
plt.show()
```
![learning_rate1](https://github.com/guofei9987/StatisticsBlog/blob/master/%E9%99%84%E4%BB%B6/tf/learning_rate1.png?raw=true)



用法案例
```py
import tensorflow as tf
TRAINING_STEPS = 100
global_step = tf.Variable(0)
LEARNING_RATE = tf.train.exponential_decay(0.1, global_step, 1, 0.96, staircase=True)

x = tf.Variable(tf.constant(5, dtype=tf.float32), name="x")
y = tf.square(x)
train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(y, global_step=global_step)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(TRAINING_STEPS):
        sess.run(train_op)
        if i % 10 == 0:
            LEARNING_RATE_value = sess.run(LEARNING_RATE)
            x_value = sess.run(x)
            print ("After %s iteration(s): x%s is %f, learning rate is %f."% (i+1, i+1, x_value, LEARNING_RATE_value))
```




## 参考文献
[TF官网-tf.nn](https://www.tensorflow.org/api_docs/python/tf/nn)  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战Google深度学习框架》，郑泽宇，中国工信出版集团  
