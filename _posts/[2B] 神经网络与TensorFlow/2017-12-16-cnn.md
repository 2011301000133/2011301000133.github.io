---
layout: post
title: 【神经网络8】CNN理论与实现
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 262
---


## 介绍

卷积神经网络(CNN,Convolutional neural network)由日本人福岛邦彦与1990s首先提出。  
CNN之父Yann LeCun于1997年提出LeNet-5,有效实现了手写识别。  
卷积神经网络的概念出自1960s科学家提出的感受野(Receptive Field)，发现每个视觉神经元只会处理一小块区域的视觉图像  


是一种前馈神经网络，与其它前馈神经网络最大的区别是，并不是全连接网络(full connect network)  
有两个大的特点：
1. 至少一个卷积层，用来提取特征
2. 卷积层权值共享，大大减少权值数量，提高收敛速度。


主要用来识别位移、缩放、其他形式扭曲不变性的二维图像。  
避免了显式的特征抽取。


也可以用于时间序列信号，例如音频信号、文本数据




## 卷积
数学中卷积的定义：$f(x) * g(x)=\int_{-\infty}^{\infty} f(t)g(x-t)d t$


神经网络中卷积有这几个参数：Padding，Striding
### Padding
边界有多少个像素，低像素通常设定为0，高像素可以设定成5~10  
目的：
1. 保持边界信息，否则边缘的像素扫描了一遍，而中间的像素扫描了多次，导致信息参考程度不同。
2. 如果输入图片的尺寸有差异，可以用Padding补齐


### Stride
每次滑动的单位，通常Stride=1，细密程度最好。  
也可以取别的值，例如Stride=3,表示每次移动3像素，粗糙了一些。


### 池化层
Pooling Layer，一些旧的CNN网络喜欢使用的一种处理层。


常见两种：
1. Max Pooling
2. Mean Pooling


在卷积的基础上，不是做加，而是求Max/Mean
## 经典CNN：LeNet-5

<img src='http://www.guofei.site/public/postimg/ann_LeNet_5.png'>  


LeCun于1997年提出


输入层→(卷积层+→池化层？)+→全连接层+  


卷积层+表示可以连续使用卷积层，一般最多连续使用三层  
池化层？表示可有可无  
## 4种现代经典CNN
ImageNet由斯坦福大学华人教授李飞飞创办，有1500万张标注过的图片，总共22000类

ILSVRC（ImageNet Large Scale Visual Recognition Challenge）比赛

<table class="tableizer-table">
<thead><tr class="tableizer-firstrow"><th>模型名</th><th>奖项</th><th>Top-5错误</th><th>层次</th></tr></thead><tbody>
 <tr><td>AlexNet</td><td>2012冠军</td><td>16.40%</td><td>8</td></tr>
 <tr><td>VGGNet</td><td>2014亚军</td><td>7.30%</td><td>19</td></tr>
 <tr><td>Google Inception Net</td><td>2014冠军</td><td>6.70%</td><td>22</td></tr>
 <tr><td>ResNet</td><td>2015冠军</td><td>3.57%</td><td>152</td></tr>
 <tr><td>人眼</td><td>&nbsp;</td><td>5.10%</td><td></td></tr>
</tbody></table>


### 1. AlexNet
Hitton的学生Alex Krizhevsky提出。
包含6亿3000万个连接，6000万个参数，65万个神经元。  
5个卷积层，其中3个卷积层连接了maxpool，最后3个全连接层。  
AlexNet在神经网络低谷期第一次发声，确立了深度卷积网络在计算机视觉中的统治地位。  


<img src='http://www.guofei.site/public/postimg/ann_AlexNet.jpeg'>  


AlexNet将LeNet的思想发扬光大。新技术点如下：
1. ReLU作为CNN的激活函数，并验证其在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。ReLU很早被提出，在AlexNet出现才开始发扬光大
2. 使用Dropout.Dropout虽然有单独论文阐述，但AlexNet将其实用化
3. 使用Maxpool，此前普遍使用Meanpool。提出步长小于池化核的尺寸，使输出有重叠和覆盖，提升特征丰富性。
4. 提出LRN层(Local Response Norm)，对局部神经元的活动创建竞争机制，提高泛化能力。
5. 使用CUGA加速训练。设计GPU通讯。
6. 数据增强。随机从$256\times 256$图像中截取$224\times 224$，并加上水平翻转，相当于增加了$(256-224)^2\times 2=2048$倍数据量。防止相对于数据，参数太多导致的过拟合


### 2. VGGNet


<img src='http://www.guofei.site/public/postimg/ann_VGG.jpeg'>  



VGGNet是牛津大学计算机视觉组(Visual Geometry Group)和Google DeepMind公司研究院合作研发的深度卷积神经网络。  
反复堆叠$3\times 3$小型卷积核，$2\times 2$ Maxpool，深度为16~19层  


卷积层串联可行的原因：  
2个$3\times 3$小型卷积核串联，感受野相当于1个$5\times 5$  
3个$3\times 3$小型卷积核串联，感受野相当于1个$7\times 7$。
前者参数3*3*3=27个，后者参数7*7=49个  
前者可以有更多非线性变换（可以每层都加上ReLU），后者需要只能加1个ReLU  


其它技巧：  
对原始数据，也使用了随机裁切  
先训练A网络，复用A网络训练更复杂的模型，这样训练速度更快  


### 3. Google Inception Net


<img src='http://www.guofei.site/public/postimg/ann_GoogleNet.jpeg'>  



最大的特点是大大减少了参数和计算量


去除了最后的全连接层，改用MeanPooling  
设计Inception Module提高参数利用效率  

### 4. ResNet


<img src='http://www.guofei.site/public/postimg/ann_ResNet.png'>  



有些资料把这个模型叫做Deep Residual Learning  
微软研究院的Kaiming He等4名华人提出，使用Residual Unit.  
参数量比VGGNet少，推广性很好  


ResNet出自这个问题：不断增加深度时，准确率先上升在下降，而且问题不是来自过拟合。ResNet成功解决了这个问题。


使用了Highway Network，使得理论上任意深的神经网络都可以直接使用梯度下降法训练。
传统神经网络的某些片段，输入x，输出$H(x)$。  
ResNet的学习目标是$F(x)=H(x)-x$,学习的目标不是完整的输出$H(x)$，而是$H(x)-x$残差。  


ResNet类似一个没有gates的LSTM网络

## TensorFlow实现
<iframe src="http://www.guofei.site/StatisticsBlog/TF_CNN.html" width="100%" height="1800em" marginwidth="10%"></iframe>


## 参考文献
《Matlab神经网络原理与实例精解》陈明，清华大学出版社   
《神经网络43个案例》王小川，北京航空航天大学出版社  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战》中国工信出版集团
