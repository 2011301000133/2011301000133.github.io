---
layout: post
title: 神经网络中的若干问题
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 450
---

## sigmoid作为激活函数的原因
### 从exponential family角度看
关于exponential family, 参见我的另一篇文章<a href='/2017/05/26/distribution1.html#title12'>常见统计分布(2).</a>  

Bernoulli distribution的概率分布函数可以写为：  
$f(x)=p^x(1-p)^{1-x}$  
进一步写为，  
$f(x)=exp[ln\dfrac{p}{1-p} x+ln(1-p)]$  

比较exponential family的定义：  
$P(y;\eta)=b(y)exp(\eta^T T(y)-a(\eta))$    

得到,
$\eta=\dfrac{p}{1-p}$


注：从normal distribution推导exponential family，对应的是线性回归。

### 从logistic regression角度看
用sigmoid function后，logistic regression的参数，MLE方法等价于OLS方法  

### 从BP神经网络角度看
误差反向传播过程中，sigmoid function有以下好处：
- 光滑：处处可导，导数连续
- 压缩：把R压缩到[-1,1],并且接近0更敏感。这是BP的需求。
- 导数容易求。y'=y(1-y)$.
误差反向传播算法中，大大减少了运算量（误差反向传播的详细介绍在我的博客中(www.guofei.site)）


## 交叉熵作为 Cost Function 的原因
残差平方和作为 Cost Function ，误差前向传播算法中，最后一项开始便有$\sigma'(v)$这一项，神经元的输入值较为极端时，$\sigma'(v)=\sigma(v) (1-\sigma(v))$ 接近0.  
用交叉熵作为 Cost Function，最后一层不存在这个问题  
公式自己推导，很简单。  


## softmax 作为最后一层的原因
1. 非负、和总是1，符合对概率的定义  
2. softmax regression 的二元情况与 logistics regression 等价
3. 与最大熵模型等价


## 过拟合的应对
1. 增加数据量往往可以有效降低过拟合的可能性
2. validation
3. L1, L2 规范化


以下引用 [Michael Nielsen](http://michaelnielsen.org/) 的说法  
规范化能够减轻过拟合，但背后的原因还不得而知。通常认为是因为小的权重意味着更低的复杂性。  
例如，一个线性模型和一个9阶多项式都可以回归一组点，有两种可能：
1. 9次多项式是正确的
2. 线性模型是正确的，噪音是因为存在测量误差


科学中，很难说明哪种原则正确，“奥卡姆剃刀”也不是一个一般的科学原理。例如，爱因斯坦的理论更能预测天体的微小偏差，但复杂很多。  
归根结底，你可以把规范化看成某种整合的技术，尽管其效果不错，但我们并没有一套完整的理解，仅仅当作不完备的启发规则或经验。规范化是一种帮助神经网络泛化的魔法，但不会带来原理上理解的指导。


4. Dropout :随机、临时关闭某些神经元。实际预测时，全部神经元激活，为了补偿这个，把权重按比例缩小。  
Dropout 类似于某种投票机制，有些类似训练不同的神经网络  
5. 认为扩展训练数据。  
第一条说了，增加数据量往往可以有效降低过拟合的可能性。  
例如，MNIST数据库中，将数据进行旋转。
