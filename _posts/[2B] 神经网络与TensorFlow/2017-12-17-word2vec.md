---
layout: post
title: word2vec
categories:
tags: 2B神经网络与TF
keywords:
description:
order: 262
---


## 介绍

### One-Hot Encoder
在word2vec出现之前，通常将词汇转化为一个单位向量。  
例如"北京"对应3987，中国对应"5178"，也就是向量对应位置上数字为1，其余为0.  
而对于一篇文章，对应的向量是所有词汇对应向量的和。  


缺点
1. 编码随机，从向量看不出两个词汇的关系。
2. 稀疏矩阵训练效率较低


### Vector Space Models
可以将字词转化为连续值  
主要依赖假设是Distributional Hypothesis，相同语境下出现的字将被映射到向量空间的相近位置。  


有两类具体的方法：
1. 计数模型,记录相邻的词出现的频率，然后把计数结果转为小而稠密的矩阵(例如Latent Semantic Analysis)
2. 预测模型，根据相邻的词，推测指定位置的词。(例如Neural Probabilistic Language Models)


## Word2Vec
Neural Probabilistic Language Models通常可以用MLE方法计算，但计算量巨大。  
Word2Vec的CBOW中，不需要计算完整概率，而是训练一个二元分类模型，用来区分真实词汇和编造的噪声词汇。    
这种编造噪声用来训练的方法叫做 **Negative Sampling**  


实际中，我们用Noise-Contrastive Estimation(NCE) Loss.  
TensorFlow里有`tf.nn.nce_loss()`可以直接实现。  



## 参考文献
《Matlab神经网络原理与实例精解》陈明，清华大学出版社   
《神经网络43个案例》王小川，北京航空航天大学出版社  
《人工神经网络原理》马锐，机械工业出版社  
白话深度学习与TensorFlow，高扬，机械工业出版社  
《TensorFlow实战》中国工信出版集团
