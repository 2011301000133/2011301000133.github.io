---
layout: post
title: 【神经网络2】误差向前传播.
categories:
tags: 2.1神经网络与TF
keywords:
description:
order: 251
---



- 字母意义
  - n 表示training set中的第n个
  - i 表示某个神经元的weight vector 中第i个
  - j 表示第j个神经元


## delta rule：linear的情况

- 平方形式的残差  
$E= 0.5*\sum\limits_{n\in training}(t_n-y_n)^2$  

- 微分  
$\dfrac{\partial E}{\partial w_i}  
=0.5*\sum\limits_n \frac{\partial y_n}{\partial w_i} \frac{dE_n}{dy_n}  
=-\sum\limits_n x_{in}(t_n-y_n)$  

- 改变weight  
$\Delta w_i=-\varepsilon \dfrac{\partial E}{\partial w_i}$  


## delta rule: sigmoid的情况

1. sigmoid的神经元是这样的  
$z=b+\sum\limits_ix_iw_i$  
$y=\dfrac{1}{1+e^{-z}}$  

2. sigmoid神经元的特点  
$\dfrac{dy}{dz}= y(1-y)$  

3. sigmoid神经元的学习过程  
$\dfrac{\partial E}{\partial w_i}=-\sum\limits_n x_{in}y_n(1-y_n)(t_n-y_n)$  

## 误差向前传播算法

### 变量定义

I表示中间层，J表示输出层
n表示第n个case/sample
v表示输出，u表示输入，故而v=g(u)，其中 g()是神经元内的函数

学习过程$w_{ij}(n+1)=\Delta w_{ij}(n)+w_{ij}(n)$

### 对于第n个case,J层情况
#### 1. 用链式法则拆分
$\dfrac{\partial e(n)}{\partial w_{ij}}
=\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}
\dfrac{\partial u_j}{\partial w_{ij}}$  
- 关于第一项  
$e=0.5\sum e_j^2$  
于是$\dfrac{\partial e}{\partial e_j}=e_j=t_j-v_j^J$  
- 关于第二项   
$e_j=t_j-v_j^J$  
于是$\dfrac{\partial e_j}{\partial v_j}=-1$  
- 关于第三项  
$v_j^J=g(u_j^J)$  
于是$\dfrac{\partial v_j}{\partial u_j}=g^1(u_j^J)$  
- 关于第四项  
$u_j^J=\sum\limits_{i\in I}w_{ij}v_i^I $  
于是$\dfrac{\partial u_j}{\partial w_{ij}}=v_i^I$  


#### 2. 学习

$\Delta w_{ij}(n)=- \eta \dfrac{\partial e(n)}{\partial w_{ij}}$  

- 用链式法则拆分  
$\Delta w_{ij}(n)=- \eta
\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}
v_i^I$  
- 定义梯度为：  
$\delta_j^J=
\dfrac{\partial e}{\partial e_j}
\dfrac{\partial e_j}{\partial v_j}
\dfrac{\partial v_j}{\partial u_j}$  


于是:  
$\Delta w_{ij}(n)=-\eta \delta_j^J v_i^I$  

### 对于第n个case，I层学习

(I层之前为M层)
$\Delta w_{mi}^I= - \eta \dfrac{\partial e}{\partial w_{mi}}$

- 用链式法则拆分  
$\dfrac{\partial e}{\partial w_{mi}}
=\dfrac{\partial e}{\partial v_i^I}
\dfrac{\partial v_i^I}{\partial u_i^I}
\dfrac{\partial u_i^I}{\partial w_{mi}}$  
- 用梯度形式改写  
$\Delta w_{mi}^I=-\eta \delta_i^I v_m^M$  
- 其中梯度为：
$\delta_i^I=
\dfrac{\partial e}{\partial v_i}
\dfrac{\partial v_i}{\partial u_i}$  


经计算，其中，

$\dfrac{\partial e}{\partial v_i^I}=\sum\limits_{j\in J}\delta_j^Jw_{ij}$
