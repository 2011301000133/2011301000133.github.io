SVM的优势是可以解决`小样本`、`非线性`、`高维`的问题  

SVM建立在统计学习理论中的`VC维理论`和`结构风险最小理论`的基础上，根据有限样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折中    
数据点是n维实空间的点，希望把这些点用一个n-1维的超平面分开。这类分类器叫做`线性分类器`  
有很多分类器符合这种要求。但是我们还希望找到最佳分割平面，也就是使得两个类的数据点间隔最大的那个平面。  

### 优点
- 通用性：能够各种函数集中构造函数
- 鲁棒性：不需要微调
- 有效性：解决实际问题最好的方法
- 计算简单，理论完善：基于VC推广理论框架
- 与传统统计理论相比，统计学习理论基本不涉及概率测度的定义和大数定律
- 建立了有限样本学习问题的统一框架
- 避免了神经网络的网络结构选择、过学习、欠学习以及局部最小值问题


## Python 实现
